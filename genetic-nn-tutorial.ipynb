{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# genetic-nn-tutorial\n",
    "Basic tutorial for training feed-forward neural networks with genetic algorithms\n",
    "\n",
    "This code uses a genetic algorithm to train a feed forward neural network to learn to approximate the cosine function.\n",
    "\n",
    "The only (necessary) dependencies are basic: numpy, random and math.\n",
    "\n",
    "Note that this is written and published only with education purposes in mind, real-world training of neural networks is generally much more efficient with backpropagation.\n",
    "\n",
    "Genetic algorithms are worth understanding because, while being time-consuming, they make no requirements for end-to-end differentiability, can handle discrete values (although not done here)  and are easy to code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we only have very basic dependencies\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our dataset\n",
    "dataset = []\n",
    "\n",
    "# Where to start the X's\n",
    "x_runner = -4.0\n",
    "increment = 0.04\n",
    "\n",
    "# Generate the dataset, we're going to learn the cos(x) function\n",
    "for i in range(200):\n",
    "    dataset.append([x_runner, math.cos(x_runner)])\n",
    "    x_runner += increment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the whole dataset\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "plt.plot(list(item[0] for item in dataset), list(item[1] for item in dataset))\n",
    "fig.suptitle('Cosine function')\n",
    "plt.xlabel('x', fontsize=18)\n",
    "plt.ylabel('cos(x)', fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets\n",
    "training_dataset = dataset[:150]\n",
    "testing_dataset = dataset[-50:]\n",
    "# Training inputs\n",
    "x_training = list(item[0] for item in training_dataset)\n",
    "# Training outputs\n",
    "y_training = list(item[1] for item in training_dataset)\n",
    "# Testing inputs\n",
    "x_testing = list(item[0] for item in testing_dataset)\n",
    "# Testing outputs\n",
    "y_testing = list(item[1] for item in testing_dataset)\n",
    "\n",
    "# Plot training and testing datasets\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "fig.suptitle('Training and testing data')\n",
    "plt.plot(x_training, y_training, color='green')\n",
    "plt.plot(x_testing, y_testing, color='blue')\n",
    "plt.xlabel('x', fontsize=18)\n",
    "plt.ylabel('cos(x)', fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset to make the training and testing sets more evenly distributed\n",
    "random.shuffle(dataset)\n",
    "\n",
    "# Redo the split\n",
    "# Split into training and testing sets\n",
    "training_dataset = dataset[:150]\n",
    "testing_dataset = dataset[-50:]\n",
    "# Training inputs\n",
    "x_training = list(item[0] for item in training_dataset)\n",
    "# Training outputs\n",
    "y_training = list(item[1] for item in training_dataset)\n",
    "# Testing inputs\n",
    "x_testing = list(item[0] for item in testing_dataset)\n",
    "# Testing outputs\n",
    "y_testing = list(item[1] for item in testing_dataset)\n",
    "\n",
    "# Normalize X values\n",
    "for i in range(len(x_training)):\n",
    "    x_training[i] = x_training[i] / (200*increment - 100*increment)\n",
    "\n",
    "for i in range(len(x_testing)):\n",
    "    x_testing[i] = x_testing[i] / (200*increment - 100*increment)\n",
    "\n",
    "\n",
    "# Plot training and testing datasets\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "fig.suptitle('Training and testing data (randomized)')\n",
    "plt.scatter(x_training, y_training, color='green')\n",
    "plt.scatter(x_testing, y_testing, color='blue')\n",
    "plt.xlabel('x', fontsize=18)\n",
    "plt.ylabel('cos(x)', fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Since we're going to be using a genetic algorithm, we need something that will give us a randomly initialized neural network with the structure we want. We will assume one hidden layer.\n",
    "def produce_neural_network(inputs_size, hidden_layer_size, outputs_size, randomize = True):\n",
    "    net = {}\n",
    "    net['inputs_size'] = inputs_size\n",
    "    net['hidden_size'] = hidden_layer_size\n",
    "    net['outputs_size'] = outputs_size\n",
    "    if randomize:\n",
    "        net['weights_in'] = np.subtract(np.random.rand(inputs_size*hidden_layer_size)*5, 2.5)\n",
    "        net['weights_out'] = np.subtract(np.random.rand(hidden_layer_size * outputs_size)*5, 2.5)\n",
    "    net['biases'] = np.subtract(np.random.rand(hidden_layer_size)*5, 2.5)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we need a way to run that neural network on inputs to generate outputs\n",
    "def run_neural_network(x, net):\n",
    "    if len(x) != net['inputs_size']:\n",
    "        raise Exception('Input dimensions do not match network structure')    \n",
    "    hidden_layer = np.add(np.multiply(x, net['weights_in']), net['biases'])\n",
    "    # Now for RELU activation\n",
    "    hidden_layer = hidden_layer * (hidden_layer > 0)\n",
    "    output_layer = np.matmul( np.matrix([hidden_layer]), np.matrix([net['weights_out']]).T)\n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genetic algorithm function: As we'll see in a little bit, we need a way to \"breed\" arrays\n",
    "def breed_arrays(mother, father, crossover, mutation):\n",
    "    if len(mother) != len(father):\n",
    "        raise Exception('Incompatible parents')\n",
    "    child = []\n",
    "    # Both parents have the same genome size and the child will have the same\n",
    "    genome_size = len(mother)\n",
    "    cutoff = random.randint(1, genome_size-1)\n",
    "    genome_index = 0\n",
    "    while len(child) < genome_size:\n",
    "        if genome_index < cutoff:\n",
    "            child.append(mother[genome_index])\n",
    "        else:\n",
    "            child.append(father[genome_index])\n",
    "        mutation_decider = random.random()\n",
    "        if mutation_decider < mutation:\n",
    "            child[genome_index] += random.random() * random.random() * (2.0 * random.random() - 1.0)\n",
    "        genome_index += 1\n",
    "    return child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genetic algorithm function: We need to breed neural networks, creating one new network from two parents\n",
    "def breed_neural_networks(mother_net, father_net, crossover = 0.6, mutation = 0.005 ):\n",
    "    if mother_net['inputs_size'] != father_net['inputs_size'] or mother_net['hidden_size'] != father_net['hidden_size'] or  mother_net['outputs_size'] != father_net['outputs_size']:\n",
    "        raise Exception('Incompatible parents')\n",
    "    child_net = produce_neural_network(mother_net['inputs_size'], mother_net['hidden_size'], mother_net['outputs_size'], randomize=False)\n",
    "    child_net['biases'] = breed_arrays(mother_net['biases'], father_net['biases'], crossover, mutation)\n",
    "    child_net['weights_in'] = breed_arrays(mother_net['weights_in'], father_net['weights_in'], crossover, mutation)\n",
    "    child_net['weights_out'] = breed_arrays(mother_net['weights_out'], father_net['weights_out'], crossover, mutation)\n",
    "    return child_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final part of the genetic stuff is a fitness function\n",
    "def evaluate_neural_network(net, input_data, output_data):\n",
    "    if len(input_data) != len(output_data):\n",
    "        raise Exception('Input and output data arrays have different lengths')\n",
    "    net['input_output'] = []\n",
    "    net_input = []\n",
    "    net_output = []\n",
    "    sumSquaredError = 0\n",
    "    for dataIndex in range(len(input_data)):\n",
    "        output = run_neural_network([ input_data[dataIndex] ], net)\n",
    "        net_input.append(input_data[dataIndex])\n",
    "        net_output.append(output)\n",
    "        sampleError = abs(output - output_data[dataIndex])\n",
    "        sumSquaredError += sampleError * sampleError\n",
    "    net['input_output'].append(net_input)\n",
    "    net['input_output'].append(net_output)   \n",
    "    # We return the sum squared error\n",
    "    return sumSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to set some parameters now\n",
    "\n",
    "# Number of networks in each generation\n",
    "generation_size = 40\n",
    "# Size of the hidden layer\n",
    "hidden_layer_size = 40\n",
    "# Maximum number of generations to cycle through\n",
    "generations_max = 2000\n",
    "# Acceptable error threshold\n",
    "acceptable_error = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go ahead and create the first generation\n",
    "current_generation = []\n",
    "sorted_by_fitness = []\n",
    "for i in range(generation_size):\n",
    "    current_generation.append(produce_neural_network(1, hidden_layer_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "print('Starting evolution')\n",
    "\n",
    "best_score_per_generation = []\n",
    "\n",
    "# Our main loop that does the evolution\n",
    "for j in range(generations_max):\n",
    "    # Evaluate the current generation\n",
    "    for k in range(generation_size):\n",
    "        current_generation[k]['sse'] = evaluate_neural_network(current_generation[k], x_training, y_training)\n",
    "    # Sort by fitness\n",
    "    sorted_by_fitness = sorted(current_generation, key=lambda k: k['sse'])\n",
    "\n",
    "    # Print the best one\n",
    "    print('Best solution in generation', j, 'has error', sorted_by_fitness[0]['sse'].item(0))\n",
    "\n",
    "    if j == generations_max-1 or (len(best_score_per_generation) > 0 and best_score_per_generation[-1] > sorted_by_fitness[0]['sse']):\n",
    "        clear_output(wait=True)\n",
    "        print('Best solution in generation', j, 'has error', sorted_by_fitness[0]['sse'].item(0))\n",
    "        if j == generations_max-1:\n",
    "            print('Evolution has finished')\n",
    "        # Plot function currently learned\n",
    "        fig = plt.figure(figsize=(12, 8))\n",
    "        plt.scatter(x_training, y_training, color='green')\n",
    "        plt.scatter(sorted_by_fitness[0]['input_output'][0], sorted_by_fitness[0]['input_output'][1], color='red')\n",
    "        fig.suptitle('Training data versus learned function')\n",
    "        plt.ylabel('cos(x)')\n",
    "        plt.xlabel('x normalized')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('Best solution in generation', j, 'has error', sorted_by_fitness[0]['sse'].item(0))\n",
    "        \n",
    "    best_score_per_generation.append(sorted_by_fitness[0]['sse'].item(0))\n",
    "\n",
    "    # Stop if we're close enough already\n",
    "    if sorted_by_fitness[0]['sse'] < acceptable_error:\n",
    "        print('Done!')\n",
    "        break\n",
    "\n",
    "    # Create a new generation and make sure we hang onto the best solution so far\n",
    "    new_generation = []\n",
    "    new_generation.append(sorted_by_fitness[0])\n",
    "    # Do breeding to create the next generation\n",
    "    for l in range (1, generation_size):\n",
    "        # Randomly pick mothers and fathers\n",
    "        mother_index = 0\n",
    "        father_index = 0\n",
    "        while mother_index == father_index:\n",
    "            mother_index = int(round((generation_size-1) * (random.random() * random.random())))\n",
    "            father_index = int(round((generation_size-1) * random.random()))\n",
    "        child = breed_neural_networks(sorted_by_fitness[mother_index], sorted_by_fitness[father_index])\n",
    "        new_generation.append(child)\n",
    "    current_generation = new_generation\n",
    "print('Evolution done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting test phase')\n",
    "\n",
    "actual_output = []\n",
    "# Let's test what we learned on the testing data\n",
    "for i in range(len(x_testing)):\n",
    "    input_value = x_testing[i]\n",
    "    desired_output_value = y_testing[i]\n",
    "    print(\"input value \" + str(input_value) + \" output value \" + str(desired_output_value))\n",
    "    result = run_neural_network([input_value], sorted_by_fitness[0])\n",
    "    actual_output.append(result)\n",
    "    print('From X=',input_value,'we wanted',desired_output_value,'and got',actual_output[i].item(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning progress\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "fig.suptitle('Results')\n",
    "plt.plot(best_score_per_generation)\n",
    "plt.xlabel('iteration', fontsize=18)\n",
    "plt.ylabel('error', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results vs \"ground truth\"\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.scatter(x_training, y_training, color='green')\n",
    "plt.scatter(x_testing, y_testing, color='blue')\n",
    "plt.scatter(x_testing, actual_output, color='red')\n",
    "fig.suptitle('Training, testing and result data')\n",
    "plt.ylabel('cos(x)')\n",
    "plt.xlabel('x normalized')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
